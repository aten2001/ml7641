{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFy6leD3ZmzS"
   },
   "source": [
    "# Fall 2019 CX4641/CS7641 Homework 1 - Christopher Fleisher\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Sep 12, Thursday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged, but each student must write his own answers and explicitly mention any collaborators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2Dz3TFIZmzT"
   },
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "In this assignment, we only have writing questions: you are asked to answer them in the markdown cells.\n",
    "\n",
    "- Graduate students are required to complete all the questions including **bouns parts**. Undergraduate students are welcome to try bouns questions and we will add them on your final grade.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You could directly type the Latex equations in the markdown cell.\n",
    "\n",
    "- Typing with Latex is highly recommended. An image scan copy of handwritten also works. If you hand write, try to be clear as much as possible. No credit may be given to unreadable handwriting.\n",
    "    \n",
    "- If you want to add any picture to your answer, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vurwpE07ZmzU"
   },
   "source": [
    "## 1 Linear Algebra (25pts + 8pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6pSyvDOZmzU"
   },
   "source": [
    "### 1.1 Determinant and Inverse of Matrix [11pts]\n",
    "Given a matrix M:\n",
    "\n",
    "$$M = \\begin{bmatrix} \n",
    "  5 & 0 & 1 \\\\ \n",
    "  6 & 1 & 2 \\\\\n",
    "  0 & 4 & 3\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Calculate the determinant of M. [5pts]\n",
    "(Calculation process required)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    |M| &=\n",
    "        5\\cdot\\begin{bmatrix}\n",
    "            1 & 2 \\\\\n",
    "            4 & 3\n",
    "          \\end{bmatrix} -\n",
    "        0\\cdot\\begin{bmatrix}\n",
    "            6 & 2 \\\\\n",
    "            0 & 3\n",
    "          \\end{bmatrix} +\n",
    "        1\\cdot\\begin{bmatrix}\n",
    "            6 & 1 \\\\\n",
    "            0 & 4 \\\\\n",
    "          \\end{bmatrix} \\\\\n",
    "        &= 5(3-8)+24 \\\\\n",
    "        &= -1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Does the inverse of M exist? If so, calculate $M^{-1}$. [6pts]\n",
    "(Calculation process required)\n",
    "\n",
    "  (**Hint:** please double check your answer and make sure $M M^{-1} = I$)\n",
    "\n",
    "    Yes, the inverse exists because $det(M) \\neq 0$. The Gauss-Jordan method may be used to find the inverse thru row operations $\n",
    "    \\left[\n",
    "    \\begin{array}{c|c}\n",
    "        M & I\n",
    "    \\end{array}\n",
    "    \\right]\\rightarrow\n",
    "    \\left[\n",
    "    \\begin{array}{c|c}\n",
    "        I & M^{-1}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "    $\n",
    "    \n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\left[\\begin{array}{c|c}\n",
    "            M & I\n",
    "        \\end{array}\\right]\n",
    "        &=\n",
    "        \\left[\\begin{array}{ccc|ccc}\n",
    "            5 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "            6 & 1 & 2 & 0 & 1 & 0 \\\\\n",
    "            0 & 4 & 3 & 0 & 0 & 1 \n",
    "        \\end{array}\\right] \\\\\n",
    "        &=\n",
    "        \\left[\\begin{array}{ccc|ccc}\n",
    "            5 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "            24 & 4 & 8 & 0 & 4 & 0 \\\\\n",
    "            0 & 4 & 3 & 0 & 0 & 1 \n",
    "        \\end{array}\\right] \\\\\n",
    "        &=\n",
    "        \\left[\\begin{array}{ccc|ccc}\n",
    "            5 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "            24 & 0 & 5 & 0 & 4 & -1 \\\\\n",
    "            0 & 4 & 3 & 0 & 0 & 1 \n",
    "        \\end{array}\\right] \\\\\n",
    "        &=\n",
    "        \\left[\\begin{array}{ccc|ccc}\n",
    "            25 & 0 & 5 & 5 & 0 & 0 \\\\\n",
    "            -1 & 0 & 0 & -5 & 4 & -1 \\\\\n",
    "            0 & 4 & 3 & 0 & 0 & 1 \n",
    "        \\end{array}\\right] \\\\\n",
    "        &=\n",
    "        \\left[\\begin{array}{ccc|ccc}\n",
    "            0 & 0 & 1 & -24 & 20 & -5 \\\\\n",
    "            1 & 0 & 0 & 5 & -4 & 1 \\\\\n",
    "            0 & 4 & 3 & 0 & 0 & 1 \n",
    "        \\end{array}\\right] \\\\\n",
    "        &=\n",
    "        \\left[\\begin{array}{ccc|ccc}\n",
    "            0 & 0 & 1 & -24 & 20 & -5 \\\\\n",
    "            1 & 0 & 0 & 5 & -4 & 1 \\\\\n",
    "            0 & 1 & 0 & 18 & -15 & 45 \n",
    "        \\end{array}\\right] \\\\\n",
    "        &=\n",
    "        \\left[\\begin{array}{ccc|ccc}\n",
    "            1 & 0 & 0 & 5 & -4 & 1 \\\\\n",
    "            0 & 1 & 0 & 18 & -15 & 45 \\\\\n",
    "            0 & 0 & 1 & -24 & 20 & -5\n",
    "        \\end{array}\\right] \\\\\n",
    "        &=\n",
    "        \\left[\\begin{array}{c|c}\n",
    "            I & M^{-1}\n",
    "        \\end{array}\\right]\n",
    "    \\end{align*}\n",
    "$$\n",
    "  \n",
    "### 1.2 Characteristic Equation [8pts] (BONUS)\n",
    "Consider the eigenvalue problem: \n",
    "  $$Ax =\\lambda x, x \\neq 0$$\n",
    "where $x$ is a non-zero eigenvector and $\\lambda$ is eigenvalue of $A$. Prove that the determinant $|A-\\lambda I|= 0$.\n",
    "\n",
    "(**Hint**: If a matrix is not full-rank (has linearly dependent columns), it is singular and non-invertible)\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        &Ax = \\lambda x \\\\\n",
    "        &Ax - \\lambda x = 0 \\\\\n",
    "        &(A - \\lambda I)x = 0 \\\\\n",
    "        &|A - \\lambda I| \\neq 0 \\Rightarrow (A-\\lambda I)^{-1} \\: \\textrm{exists} \\\\\n",
    "        &x = (A - \\lambda I)^{-1} \\cdot 0 \\\\\n",
    "        &x = 0\n",
    "    \\end{align*}\n",
    "$$\n",
    "By contradiction, $|A-\\lambda I|=0$.\n",
    "        \n",
    "\n",
    "### 1.3 Eigenvalue [7pts]\n",
    "Following 1.2, given a matrix $A$:\n",
    "   $$A = \n",
    "   \\begin{bmatrix} \n",
    "    1 & r \\\\ \n",
    "    r & 1 \n",
    "    \\end{bmatrix}$$ \n",
    "    \n",
    "Calculate all the eigenvalues of $A$. (Calculation process required. Your answer should be expressed as a function of $r$.)\n",
    "\n",
    "The eigenvalues may be found by solving $det(A-\\lambda I)=0$.\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        A-\\lambda I &=\n",
    "        \\begin{bmatrix}\n",
    "            1 & r \\\\\n",
    "            r & 1\n",
    "        \\end{bmatrix} -\n",
    "        \\lambda\n",
    "        \\begin{bmatrix}\n",
    "            1 & 0 \\\\\n",
    "            0 & 1\n",
    "        \\end{bmatrix} \\\\\n",
    "        &=\n",
    "        \\begin{bmatrix}\n",
    "            1-\\lambda & r \\\\\n",
    "            r & 1-\\lambda\n",
    "        \\end{bmatrix}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        |A-\\lambda I| &= (1-\\lambda)^{2} - r^{2} \\\\\n",
    "        0 &= (1-\\lambda)^{2} - r^{2} \\\\\n",
    "        r &= \\pm (1-\\lambda) \\\\\n",
    "        \\lambda &= 1-r \\vee \\lambda = 1+r\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "### 1.4 Eigenvector [7pts]\n",
    "Following 1.3, given that the $l_2$ norm of each eigenvector is 1, what are the eigenvectors of matrix $A$? For example, if an eigenvector is \n",
    "    ${v}=\\begin{bmatrix} \n",
    "    x1 \\\\ \n",
    "    x2 \n",
    "    \\end{bmatrix}$, then $||v||_2 = \\sqrt{x_1^2 + x_2^2} = 1$ (Calculation process required.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7D5eQGGiZmzV"
   },
   "source": [
    "## 2 Expectation, Co-variance and Independence [25pts + 5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMqPtRogZmzV"
   },
   "source": [
    "Suppose $X, Y$ and $Z$ are three different random variables.\n",
    "Let $X$ obeys Bernouli Distribution. The probability disbribution function is\n",
    "    $$p(x)=\\left\\{\n",
    "    \\begin{array}{c l}\t\n",
    "         0.5 & x = c\\\\\n",
    "         0.5 & x = -c.\n",
    "    \\end{array}\\right.$$\n",
    "    $c$ is a constant here.\n",
    "Let $Y$ obeys the standard Normal (Gaussian) distribution, which can be written as $Y \\sim N(0,1)$. $X$ and $Y$ are independent. Meanwhile, let $Z = XY$.\n",
    "\n",
    "- What is the Expectation and Variance of $X$? (in terms of $c$) [4pts]\n",
    "$$\n",
    "\\begin{align*}\n",
    "E[X] &= c \\cdot p(X=c) - c \\cdot p(X=-c) = 0.5c - 0.5c = 0 \\\\\n",
    "Var(X) &= E[X^{2}] - (E[X])^{2} = E[X^{2}] = c^{2} \\cdot p(X=c) + c^{2} \\cdot p(X=-c) = c^{2} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Show that when $c=1$, $Z$ is a standard Normal (Gaussian) distribution, which means $Z \\sim N(0,1)$. [9pts]\n",
    "$$\n",
    "\\begin{align*}\n",
    "E[Z] &= E[XY] = E[cY] = cE[Y] = E[Y] \\\\\n",
    "Var(Z) &= Var(XY) = Var(cY) = c^{2}Var(Y) = Var(Y) \\\\\n",
    "Y \\sim N(0,1) & \\Rightarrow Z \\sim N(0,1)\n",
    "\\end{align*}\n",
    "$$\n",
    "- How should we choose $c$ such that Y and Z are uncorrelated (which means $Cov(Y,Z) = 0$)? [9pts]\n",
    "\n",
    "We leverage the fact that $E[Y]$ and $E[Z]$ are zero and solve for when $Cov(X, Z)=E[Y]$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "&Cov(Y,Z) = E[(Y-E[Y])(Z-E[Z])] = E[YZ] = E[XY^{2}] \\\\\n",
    "&XY^{2} = Y \\\\\n",
    "&X = Y^{-1} \\\\\n",
    "&c = Y^{-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Are Y and Z independent? (Just clarify) [3pts]\n",
    "\n",
    "No, because the value of Y impacts the value of Z. More formally, $P(Z|Y) \\neq P(Z)$.\n",
    "- Show your conclusion for the above question with an example. **(Bouns)** [5pts]\n",
    "\n",
    "Let $Y=0$ such that $P(Z=0|Y=0)=P(XY=0|Y=0)=1$. If we weren't given $Y$ then $P(Z=0)=P(XY=0)=P(Y=0) \\neq 1$ since $Y \\sim N(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHsbeDGNZmzX"
   },
   "source": [
    "## 3 Maximum Likelihood [25pts + 10pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSEe9QB9ZmzY"
   },
   "source": [
    "### 3.1 Discrete Example [15pts]\n",
    "Suppose you are playing two unfair coins. The probability of tossing a head is $2 \\theta$ for coin 1, and $\\theta$ for coin 2. You toss each coin for several times, and you get the following results:\n",
    "\n",
    "| Coin No. | Result    |\n",
    "|------|------|\n",
    "|   1  | head |\n",
    "|   2  | head |\n",
    "|   1  | tail |\n",
    "|   2  | tail |\n",
    "|   1  | head |\n",
    "|   2  | tail |\n",
    "\n",
    "- What is the probability of tossing a tail for coin 1 ($p_{t1}$) and tossing a tail for coin 2 ($p_{t2}$) [3pts]? \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_{t1} &= 1-2\\theta \\\\\n",
    "p_{t2} &= 1-\\theta\n",
    "\\end{align*}\n",
    "$$\n",
    "- What is the likelihood of the data given $\\theta$ [6pts]?\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(H_1, H_2, T_1, T_2, H_1, T_2|\\theta) &= p_{H1}^{\\sum{x_{i1}}}(1-p_{H1})^{\\sum{1-x_{i1}}}p_{H2}^{\\sum{x_{i2}}}(1-p_{H2})^{\\sum{1-x_{i2}}} \\\\\n",
    "&= (2\\theta)^{2}(1-2\\theta)\\theta(1-\\theta)^{2} \\\\\n",
    "&=(4\\theta^{3}-8\\theta^{4})(1-\\theta)^{2} \\\\\n",
    "&=4\\theta^{3}-16\\theta^{4}+20\\theta^{5}-8\\theta^{6}\n",
    "\\end{align*}\n",
    "$$\n",
    "- What is maximum likelihood estimation for $\\theta$ [6pts]?\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\theta) &= \\log(2\\theta)\\sum{x_{i1}} + \\log(1-2\\theta)\\sum{(1-x_{i1})} + \\log\\theta\\sum{x_{i2}} + \\log(1-\\theta)\\sum{(1-x_{i2})} \\\\\n",
    "\\frac{\\partial L}{\\partial \\theta} &= \\frac{\\sum{x_{i1}}}{2\\theta}-\\frac{\\sum{(1-x_{i1})}}{1-2\\theta}+\\frac{\\sum{x_{i2}}}{\\theta}-\\frac{\\sum{(1-x_{i2})}}{1-\\theta} \\\\\n",
    "\\frac{\\partial L}{\\partial \\theta} &= \\frac{1}{\\theta} - \\frac{1}{1-2\\theta}+\\frac{1}{\\theta}-\\frac{2}{1-\\theta} \\\\\n",
    "0 &= \\frac{2}{\\theta}-\\frac{1}{1-2\\theta}-\\frac{2}{1-\\theta} \\\\\n",
    "0 &= 2(1-2\\theta)(1-\\theta)-\\theta(1-\\theta)-2\\theta(1-2\\theta) \\\\\n",
    "0 &= 9\\theta^{2}-9\\theta+2 \\\\\n",
    "\\theta &= \\frac{9 \\pm (81-4(9)(2))^{\\frac{1}{2}}}{2(9)} \\\\\n",
    "\\theta &= \\frac{9 \\pm 3}{18} \\\\\n",
    "\\theta &= \\frac{2}{3} \\vee \\frac{1}{3} \\quad \\textrm{reject } \\frac{2}{3}\\\\\n",
    "\\theta &= \\frac{1}{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### 3.2 Continues Example [10pts] (BONUS)\n",
    "\n",
    "A uniform distribution in the range of $[a, b]$ is given by\n",
    "\n",
    "$$\n",
    "f(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{b-a}} & {a \\leq x \\leq b} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "What is maximum likelihood estimation for $a$ and $b$?\n",
    "(You need to show the derivation of your answer.)\n",
    "\n",
    "( **Hint**: Think of two cases, where $x < max(x_1, x_2, ..., x_n)$ and $x \\ge max(x_1, x_2, ..., x_n).)$\n",
    "\n",
    "\n",
    "Let us define an indicator function and the loss function as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(x) &= \\left\\{\\begin{array}{ll}{1} & {a \\leq x \\leq b} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\\\\\n",
    "L(a,b|x_1,x_2,...,x_n) &= \\Pi_{i=1}^{n}\\frac{1}{b-a}I(x_i) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Maximizing the loss function is equivalent to making $I(x_i)=1$ and $b-a$ as small as possible. This will occur when $a \\leq x_i \\leq b$, $a \\leq min(x_1, x_2, ..., x_n)$, and $b \\geq max(x_1, x_2, ..., x_n)$. Therefore, the MLE for $a$ is the minimum observation value and the MLE for b is the maximum observation value.\n",
    "\n",
    "### 3.3 Maximum A Posteriori (MAP) [10pts]\n",
    "<img src = \"https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png\" style=\"width:400px;height:600px\"/>\n",
    "\n",
    "(Reference: xkcd.com)\n",
    "\n",
    "Suppose there exists an unknown parameter $\\theta$ that describe whether the sun will explode tomorrow. $\\theta = 1$ means the sun will explode and $\\theta = 0$ if it won't. The likelihood function is:\n",
    "$$\n",
    "P(yes|\\theta)=\\left\\{\\begin{array}{ll}{1/36} & {\\theta = 0} \\\\ {35/36} & {\\theta = 1}\\end{array}\\right.\n",
    "$$\n",
    "- What is the maximum likelihood estimate of $\\theta$?[3pts]\n",
    "\n",
    "    The MLE of $\\theta$ is 1 since $P(yes|\\theta=1)>P(yes|\\theta=0)$\n",
    "\n",
    "- Maximum A Posteriori (MAP) estimator aims to maximize the value of $\\theta$ in $p(\\theta|yes)$. What is the MAP estimate of $\\theta$ given that $P(\\theta=0)\\gg P(\\theta=1)$? Comment on the result.[7pts]\n",
    "\n",
    "( **Hint**: You can use Bayes Rule to get $p(\\theta|yes)$ from the likelihood! )\n",
    "\n",
    "To know more about MAP, refer to wiki page:\n",
    "https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\n",
    "\n",
    "\n",
    "$\\quad \\hat{\\theta}_{map}(x) = argmax_{\\theta}f(x|\\theta)g(\\theta)$ where $g(\\theta)$ is density function of $\\theta$. Since $P(\\theta=0)>>P(\\theta=1)$, $g(\\theta=0) >> g(\\theta=1)$. Since $g(\\theta=0)-g(\\theta=1)>>f(x|\\theta=1)-f(x|\\theta=0)$, the MAP estimate of $\\theta$ is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ym8gpKMHZmzY"
   },
   "source": [
    "## 4 Information Theory [25pts + 7pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HSWti__DYx9"
   },
   "source": [
    "### 4.1 Marginal Distribution [4pts]\n",
    "Suppose the joint probability distribution of two binary random variables $X$ and $Y$ are given as follows.\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\\hline X | Y & {1} & {2} \\\\ \\hline 0 & {\\frac{1}{5}} & {\\frac{2}{5}} \\\\ \\hline 1 & {0} & \\frac{2}{5} \\\\ \\hline\\end{array}\n",
    "$$\n",
    "\n",
    "- Show the marginal distribution of $X$ and $Y$, respectively. [4pts]\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&P(X=x)=\\sum_{y}{P(X=x,Y=y)}\\\\\n",
    "&P(X=0)=\\frac{1}{5}+\\frac{2}{5}=\\frac{3}{5} \\\\\n",
    "&P(X=1)=\\frac{2}{5} \\\\\n",
    "&P(Y=y)=\\sum_{x}{P(Y=y,X=x)} \\\\\n",
    "&P(Y=1)=\\frac{1}{5} \\\\\n",
    "&P(Y=2)=\\frac{2}{5}+\\frac{2}{5}=\\frac{4}{5}\n",
    "\\end{align*}\n",
    "$$\n",
    "    \n",
    "### 4.2 Mutual Information and Entropy [21pts]\n",
    "Given a dataset as below.\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\\hline Day & Outlook & Temperature & Humidity & Wind & Play? \\\\ \\hline 1 & overcast & hot & normal & medium & yes \\\\ \\hline 2 & sunny & hot & high & weak & no \\\\ \\hline 3 & sunny & mild & normal & weak & yes \\\\ \\hline 4 & rain & cool & high & strong & no \\\\ \\hline 5 & overcast & cool & normal & strong & yes \\\\ \\hline 6 & rain & mild & normal & medium & no \\\\ \\hline 7 & sunny & mild & high & medium & yes\\\\ \\hline 8 & overcast & hot & normal & strong & no\\\\ \\hline 9 & rain & hot & high & weak & no\\\\ \\hline 10 & sunny & cool & normal & strong & yes\\\\\\hline\\end{array}\n",
    "$$\n",
    "\n",
    "We want to decide whether to play or not to play basketball on a certain day. Each input has four features ($x_1$, $x_2$, $x_3$, $x_4$): Outlook, Temperature, Humidity, Wind. The decision (play vs no-play) is represented as $Y$.\n",
    "\n",
    "- Find entropy $H(Y)$. [4pts]\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(Y) &= -\\sum_{k=1}^{K}{P(Y=k)\\log_{2}P(Y=k)} \\\\\n",
    "&= -(0.5\\log_{2}{0.5}+0.5\\log_{2}{0.5}) \\\\\n",
    "&= -\\log_{2}0.5 \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "- Find conditional entropy $H(Y|x_1)$, $H(Y|x_4)$, respectively. [8pts]\n",
    "  \n",
    "$$\n",
    "\\begin{align*}\n",
    "H(Y|x_1) &= \\sum_{x_1 \\in X_1, y \\in Y}{P(x_1,y)\\log{\\frac{P(x_1)}{P(x_1, y)}}} \\\\\n",
    "&= P(overcast, yes)\\log{\\frac{P(overcast)}{P(overcast, yes)}} + P(overcast, no)\\log{\\frac{P(overcast)}{P(overcast, no)}} ... \\\\\n",
    "&= 0.2\\log{\\frac{0.3}{0.2}}+0.1\\log{\\frac{0.3}{0.1}}+0.3\\log{\\frac{0.4}{0.3}}+0.1\\log{\\frac{0.4}{0.1}}+0.3\\log{\\frac{0.3}{0.3}} \\\\\n",
    "&= 0.6 \\\\\n",
    "H(Y|x_4) &= \\sum_{x_4 \\in X_4, y \\in Y}{P(x_4,y)\\log{\\frac{P(x_4)}{P(x_4, y)}}} \\\\\n",
    "&= P(medium, yes)\\log{\\frac{P(medium)}{P(medium, yes)}} + P(medium, no)\\log{\\frac{P(medium)}{P(medium, no)}} ... \\\\\n",
    "&= 0.2\\log{\\frac{0.3}{0.2}}+0.1\\log{\\frac{0.3}{0.1}}+0.1\\log{\\frac{0.3}{0.1}}+0.2\\log{\\frac{0.3}{0.2}}+0.2\\log{\\frac{0.4}{0.2}}+0.2\\log{\\frac{0.4}{0.2}}\\\\\n",
    "&= 0.4\\log{1.5}+0.2\\log{3}+0.4\\log{2} \\\\\n",
    "&= 0.9509775 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "- Find mutual information $I(x_1, Y)$ and $I(x_4, Y)$ and determine whether which one ($x_1$ or $x_4$) is more informative. [5pts]\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(x_1,Y) &= H(Y)-H(Y|x_1) \\\\\n",
    "&= 1.0-0.6 \\\\\n",
    "&= 0.4 \\\\\n",
    "I(x_4,Y) &= H(Y)-H(Y|x_4) \\\\\n",
    "&= 1.0 - 0.9509975 \\\\\n",
    "&= 0.049 \\\\\n",
    "I(x_1,Y) &> I(x_4,Y) \\Rightarrow x_1 \\textrm{ is more informative than } x_4 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Find joint entropy $H(Y, x_3)$. [4pts]\n",
    "  \n",
    "$$\n",
    "\\begin{align*}\n",
    "H(Y,x_3) &= \\sum_{y,x_3}{P(Y=y,X_{3}=x_{3})\\log{\\frac{1}{P(Y=y,X_{3}=x_{3})}}} \\\\\n",
    "&= H((normal, yes), (high,no), (normal, no), (high, yes)) \\\\\n",
    "&= H(0.4, 0.3, 0.2, 0.1) \\\\\n",
    "&= 0.4\\log{\\frac{1}{0.4}}+0.3\\log{\\frac{1}{0.3}}+0.2\\log{\\frac{1}{0.2}}+0.1\\log{\\frac{1}{0.1}} \\\\\n",
    "&= 1.846439\n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "### 4.3 Bonus Question [7pts]\n",
    "- Suppose $X$ and $Y$ are independent. Show that $H(X|Y) = H(X)$. [2pts]\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(X,Y) &= H(X)-H(X|Y) \\\\\n",
    "0 &= H(X)-H(X|Y) \\\\\n",
    "H(X|Y) &= H(X)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Suppose $X$ and $Y$ are independent. Show that $H(X,Y) = H(X) + H(Y)$. [2pts]\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(X,Y) &= H(Y)+H(X|Y) \\\\\n",
    "H(X,Y) &= H(Y)+H(X) \\quad \\textrm{via substitution using result found above}\n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "- Prove that the mutual information is symmetric, i.e., $I(X, Y) = I(Y, X)$ and $x_i \\in X, y_i \\in Y$ [3pts]\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(X,Y) &= H(X)-H(X|Y) \\\\\n",
    "&= \\sum_{x}{p(x)\\log{\\frac{1}{p(x)}}}-\\sum_{x,y}{p(x,y)\\log{\\frac{1}{p(x|y)}}} \\\\\n",
    "&= \\sum_{x,y}{p(x,y)\\log{p(x|y)}}-\\sum_{x}{p(x)\\log{p(x)}} \\\\\n",
    "&= \\sum_{x,y}{p(x,y)\\log{p(x|y)}}-\\sum_{x}{(\\sum_{y}{p(x,y)})\\log{p(x)}} \\\\\n",
    "&= \\sum_{x,y}{p(x,y)\\log{p(x|y)}}-\\sum_{x,y}{p(x,y)\\log{p(x)}} \\\\\n",
    "&= \\sum_{x,y}{p(x,y)[\\log{p(x|y)}-\\log{p(x)}]} \\\\\n",
    "&= \\sum_{x,y}{p(x,y)\\log{\\frac{p(x|y)}{p(x)}}} \\\\\n",
    "&= \\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}} \\\\\n",
    "I(Y,X) &= H(Y)-H(Y|X) \\\\\n",
    "&= \\sum_{y}{p(y)\\log{\\frac{1}{p(y)}}}-\\sum_{y,x}{p(y,x)\\log{\\frac{1}{p(y|x)}}} \\\\\n",
    "&= \\sum_{y,x}{p(y,x)\\log{p(y|x)}}-\\sum_{y}{p(y)\\log{p(y)}} \\\\\n",
    "&= \\sum_{y,x}{p(y,x)\\log{p(y|x)}}-\\sum_{y}{(\\sum_{y}{p(y,x)})\\log{p(y)}} \\\\\n",
    "&= \\sum_{y,x}{p(y,x)\\log{p(y|x)}}-\\sum_{y,x}{p(y,x)\\log{p(y)}} \\\\\n",
    "&= \\sum_{y,x}{p(y,x)[\\log{p(y|x)}-\\log{p(y)}]} \\\\\n",
    "&= \\sum_{y,x}{p(y,x)\\log{\\frac{p(y|x)}{p(y)}}} \\\\\n",
    "&= \\sum_{x,y}{p(x,y)\\log{\\frac{p(x,y)}{p(x)p(y)}}} \\\\\n",
    "I(X,Y) &= I(Y,X)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1-template.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
