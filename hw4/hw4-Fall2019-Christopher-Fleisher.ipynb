{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hB8uQCzVs6VI"
   },
   "source": [
    "# Fall 2019 CX4641/CS7641 Homework 4\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Dec 3, Tuesday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged, but each student must write his own answers and explicitly mention any collaborators.\n",
    "\n",
    "* Homework submission ONLY in .ipynb format. You don't need to submit the image.\n",
    "\n",
    "* Throughout the ipython notebook, we use attribute and feature interchangeably.\n",
    "\n",
    "* Graduate students are required to answer all the questions including bonus parts. Bonus points are just for undergraduate students.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7U0WVt07tGRv"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0Ui6T2as9iI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from math import log2, sqrt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdkkppO-pNEn"
   },
   "source": [
    "## Part 1: Utility Functions [25pts]\n",
    "\n",
    "Here, we ask you to develop a few functions that will be the main building blocks of your decision tree and random forest algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e_A6GBe11nA"
   },
   "source": [
    "### Entropy and information gain [10pts]\n",
    "\n",
    "First, we computes entropy and then use this entropy for information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVcXl9-D2DMW"
   },
   "outputs": [],
   "source": [
    "def entropy(class_y):\n",
    "    \"\"\"\n",
    "    E = -sum p*log2(p)\n",
    "    Input: \n",
    "        - class_y: list of class labels (0's and 1's)\n",
    "    \n",
    "    TODO: Compute the entropy for a list of classes\n",
    "    Example: entropy([0,0,0,1,1,1,1,1]) = 0.9544\n",
    "    \"\"\"\n",
    "    labels = np.array(class_y)\n",
    "    cnt = labels.shape[0]\n",
    "    vals = [i for i in [0,1] if i in labels]\n",
    "    probs = np.array([(labels == v).sum()/cnt for v in vals])\n",
    "    probs *= -np.log2(probs)\n",
    "    return probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wp7wEfZ82Owg"
   },
   "outputs": [],
   "source": [
    "def information_gain(previous_y, current_y):\n",
    "    \"\"\"\n",
    "    IG = E(parent)- weighted mean E(child)\n",
    "    Inputs:\n",
    "        - previous_y : the distribution of original labels (0's and 1's)\n",
    "        - current_y  : the distribution of labels after splitting based on a particular\n",
    "                     split attribute and split value\n",
    "    \n",
    "    TODO: Compute and return the information gain from partitioning the previous_y labels into the current_y labels.\n",
    "    \n",
    "    Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf \n",
    "\n",
    "    Example: previous_y = [0,0,0,1,1,1], current_y = [[0,0], [1,1,1,0]], info_gain = 0.4591\n",
    "    \"\"\" \n",
    "    cnt = len(previous_y)\n",
    "    return entropy(previous_y)-np.array([len(c)/cnt*entropy(c) for c in current_y]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-nbf6IJ2fkS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.954434002924965\n",
      "0.4591479170272448\n"
     ]
    }
   ],
   "source": [
    "# TEST CASE\n",
    "test_class_y = [0,0,0,1,1,1,1,1]\n",
    "print(entropy(test_class_y))\n",
    "\n",
    "previous_y = [0,0,0,1,1,1]\n",
    "current_y = [[0,0], [1,1,1,0]] \n",
    "print(information_gain(previous_y, current_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-xDzI702jxv"
   },
   "source": [
    "### Build a simple desicion tree step by step [15pts]\n",
    "\n",
    "Now we will implement three functions to build a decision tree from the scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mBJIZFd2lx4"
   },
   "source": [
    "#### (1) partition_classes: [5pts]\n",
    "\n",
    "One of the basic operations is to split a tree on one attribute (features) with a specific value for that attribute.\n",
    "\n",
    "In partition_classes(), we split the data (X) and labels (y) based on the split feature and value - BINARY SPLIT.\n",
    "\n",
    "You will have to first check if the split attribute is numerical or categorical. If the split attribute is numeric, split_val should be a numerical value. For example, your split_val should go over all the values of attributes. If the split attribute is categorical, split_val should include all the categories one by one.   \n",
    "    \n",
    "You can perform the partition in the following way:\n",
    "   - Numeric Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "       the rows where the split attribute is less than or equal to the split value, and the \n",
    "       second list has all the rows where the split attribute is greater than the split \n",
    "       value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "    \n",
    "   - Categorical Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all \n",
    "       the rows where the split attribute is equal to the split value, and the second list\n",
    "       has all the rows where the split attribute is not equal to the split value.\n",
    "       Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-Ch02qB2oJm"
   },
   "outputs": [],
   "source": [
    "def partition_classes(X, y, split_attribute, split_val):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X               : (N,D) list containing all data attributes\n",
    "    - y               : a list of labels\n",
    "    - split_attribute : column index of the attribute to split on\n",
    "    - split_val       : either a numerical or categorical value to divide the split_attribute\n",
    "    \n",
    "    TODO: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X = [[3, 'aa', 10],                 y = [1,\n",
    "         [1, 'bb', 22],                      1,\n",
    "         [2, 'cc', 28],                      0,\n",
    "         [5, 'bb', 32],                      0,\n",
    "         [4, 'cc', 32]]                      1]\n",
    "    \n",
    "    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n",
    "    \n",
    "    Consider the case where we call the function with split_attribute = 0 (the index of attribute) and split_val = 3 (the value of attribute).\n",
    "    Then we divide X into two lists - X_left, where column 0 is <= 3 and X_right, where column 0 is > 3.\n",
    "    \n",
    "    X_left = [[3, 'aa', 10],                 y_left = [1,\n",
    "              [1, 'bb', 22],                           1,\n",
    "              [2, 'cc', 28]]                           0]\n",
    "              \n",
    "    X_right = [[5, 'bb', 32],                y_right = [0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "\n",
    "    Consider another case where we call the function with split_attribute = 1 and split_val = 'bb'\n",
    "    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n",
    "        \n",
    "    X_left = [[1, 'bb', 22],                 y_left = [1,\n",
    "              [5, 'bb', 32]]                           0]\n",
    "              \n",
    "    X_right = [[3, 'aa', 10],                y_right = [1,\n",
    "               [2, 'cc', 28],                           0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "               \n",
    "    Return in this order: (X_left, X_right, y_left, y_right)           \n",
    "    \"\"\"\n",
    "    dfx = pd.DataFrame(X)\n",
    "    y = np.array(y)\n",
    "    if isinstance(split_val, int):\n",
    "        # numerical\n",
    "        lmask = dfx.iloc[:, split_attribute] <= split_val\n",
    "    else:\n",
    "        # categorical\n",
    "        lmask = dfx.iloc[:, split_attribute] == split_val\n",
    "    return dfx[lmask].values, dfx[~lmask].values, y[lmask], y[~lmask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpsToj_T2x42"
   },
   "source": [
    "#### (2) find_best_split [5pts]\n",
    "\n",
    "Given the data and labels, we need to find the order of splitting features, which is also the importance of the feature. For each attribute (feature), we need to calculate its optimal split value along with the corresponding information gain and then compare with all the features to find the optimal attribute to split.\n",
    "\n",
    "First, we specify an attribute. After computing the corresponding information gain of each value at this attribute list, we can get the optimal split value, which has the maximum information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nG3Def4L20re"
   },
   "outputs": [],
   "source": [
    "def find_best_split(X, y, split_attribute):\n",
    "    \"\"\"Inputs:\n",
    "        - X               : (N,D) list containing all data attributes\n",
    "        - y               : a list array of labels\n",
    "        - split_attribute : Column of X on which to split\n",
    "    \n",
    "    TODO: Compute and return the optimal split value for a given attribute, along with the corresponding information gain\n",
    "    \n",
    "    Note: You will need the functions information_gain and partition_classes to write this function\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           split_val = 1  -->  info_gain = 0.17\n",
    "           split_val = 2  -->  info_gain = 0.02\n",
    "           split_val = 3  -->  info_gain = 0.02\n",
    "           split_val = 4  -->  info_gain = 0.32\n",
    "           split_val = 5  -->  info_gain = 0.\n",
    "        \n",
    "       best_split_val = 4; info_gain = .32;\n",
    "    \"\"\"\n",
    "    split_vals = np.unique(pd.DataFrame(X).iloc[:, split_attribute])\n",
    "    partitions = [partition_classes(X, y, split_attribute, sv) for sv in split_vals]\n",
    "    igs = np.array([information_gain(y, [p[2], p[3]]) for p in partitions])\n",
    "    return split_vals[igs.argmax()], igs.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8eM_fLu3GN9"
   },
   "source": [
    "#### (3)  find_best_feature [5pts]\n",
    "\n",
    "Based on the above functions, we can find the most important feature that we will split first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "253k0w6Y3ISy"
   },
   "outputs": [],
   "source": [
    "def find_best_feature(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X: (N,D) list containing all data attributes\n",
    "        - y : a list of labels\n",
    "    \n",
    "    TODO: Compute and return the optimal attribute to split on and optimal splitting value\n",
    "    \n",
    "    Note: If two features tie, choose one of them at random\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           feature 0:  -->  info_gain = 0.32\n",
    "           feature 1:  -->  info_gain = 0.17\n",
    "           feature 2:  -->  info_gain = 0.42\n",
    "        \n",
    "        best_split_feature: 2 best_split_val: 22\n",
    "    \"\"\"\n",
    "    split_features = range(len(X[0]))\n",
    "    split_vals, igs = zip(*[find_best_split(X, y, feat) for feat in split_features])\n",
    "    best_idx = np.array(igs).argmax()\n",
    "    return split_features[best_idx], split_vals[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0IBRUsW3P-7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[3, 'aa', 10],\n",
      "       [1, 'bb', 22],\n",
      "       [2, 'cc', 28]], dtype=object), array([[5, 'bb', 32],\n",
      "       [4, 'cc', 32]], dtype=object), array([1, 1, 0]), array([0, 1]))\n",
      "(array([[1, 'bb', 22],\n",
      "       [5, 'bb', 32]], dtype=object), array([[3, 'aa', 10],\n",
      "       [2, 'cc', 28],\n",
      "       [4, 'cc', 32]], dtype=object), array([1, 0]), array([1, 0, 1]))\n",
      "best_split_val: 2 info_gain: 0.3219280948873623\n",
      "best_split_feature: 0 best_split_val: 2\n"
     ]
    }
   ],
   "source": [
    "# TEST CASE\n",
    "test_X = [[3, 'aa', 10],[1, 'bb', 22],[2, 'cc', 28],[5, 'bb', 32],[4, 'cc', 32]]\n",
    "test_y = [1,1,0,0,1]\n",
    "print(partition_classes(test_X, test_y, 0, 3))\n",
    "print(partition_classes(test_X, test_y, 1, 'bb'))\n",
    "\n",
    "split_attribute = 0\n",
    "best_split_val, info_gain = find_best_split(test_X, test_y, split_attribute)\n",
    "print(\"best_split_val:\", best_split_val, \"info_gain:\", info_gain)\n",
    "\n",
    "best_feature, best_split_val = find_best_feature(test_X, test_y)\n",
    "print(\"best_split_feature:\", best_feature, \"best_split_val:\", best_split_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayv_tqnyxMb0"
   },
   "source": [
    "# Part 2: Decision Tree [20 pts]\n",
    "## Please read the following instructions carefully before you dive into coding\n",
    "\n",
    "In this part, you will implement your own ID3 decision tree class and make it work on training and test set.\n",
    "\n",
    "You may use a recursive way to construct the tree and make use of helper functions in Part1. \n",
    "\n",
    "Please keep in mind that we use information gain to find the best feature and value to split the data for ID3 tree.\n",
    "\n",
    "To save your training time, we have added a ```max_depth``` parameter to control the maximum depth of the tree. You may adjust its value to pre-pruned the tree. If set to None, it has no control of depth.\n",
    "\n",
    "You need to have a stop condition for splitting. This can be like, all labels in the current node are the same or reaching the pre-defined max_depth.\n",
    "\n",
    "The MyDecisionTree class should have some member variables. We highly encourage you to use a list in Python to store the tree information. For leaves nodes, this list may have just one element representing the class label. For non-leaves node, the list should at least store the feature and value to split, and references to its left and right child.\n",
    "\n",
    "### If you use different ways to represent and store the information, please include clear comments or documentations with your code. If your result if not correct, partial credits can only be awarded if we are able to understand your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2Cwef24xgtT"
   },
   "outputs": [],
   "source": [
    "class MyDecisionTree(object):\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        TODO: Initializing the tree as an empty dictionary or list, as preferred.\n",
    "        \n",
    "        For example: self.tree = [] or self.tree = {}\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = []\n",
    "\n",
    "    def fit(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        TODO: Train the decision tree (self.tree) using the the sample X and labels y.\n",
    "        \n",
    "        NOTE: You will have to make use of the utility functions to train the tree.\n",
    "        One possible way of implementing the tree: Each node in self.tree could be in the form of a dictionary:\n",
    "        https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        \n",
    "        For example, a non-leaf node with two children can have a 'left' key and  a  'right' key. \n",
    "        You can add more keys which might help in classification (eg. split attribute and split value)\n",
    "        \"\"\"\n",
    "        # return random label if no examples\n",
    "        if len(y) == 0:\n",
    "            return [np.random.randint(2)]\n",
    "        \n",
    "        # return any label if all labels the same\n",
    "        if all(label == y[0] for label in y):\n",
    "            return [y[0]]\n",
    "        \n",
    "        # return majority label if max depth reached\n",
    "        if self.max_depth is not None and depth > self.max_depth:\n",
    "            if sum(y) > len(y)/2:\n",
    "                return [1]\n",
    "            else:\n",
    "                return [0]\n",
    "        \n",
    "        # pick best split feature and value\n",
    "        split_feature, split_val = find_best_feature(X, y)\n",
    "        \n",
    "        # split the current tree\n",
    "        Xl, Xr, yl, yr = partition_classes(X, y, split_feature, split_val)\n",
    "        tree = [split_feature, split_val, self.fit(Xl, yl, depth+1), self.fit(Xr, yr, depth+1)]\n",
    "        if depth == 0:\n",
    "            self.tree = tree\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "    def predict(self, record):\n",
    "        \"\"\"\n",
    "        TODO: classify a sample in test data set using self.tree and return the predicted label\n",
    "        \"\"\"\n",
    "        tree = self.tree\n",
    "        while True:\n",
    "            # check if leaf\n",
    "            if len(tree) == 1:\n",
    "                break\n",
    "            \n",
    "            # recursively continue down tree\n",
    "            split_feature, split_val, ltree, rtree = tree\n",
    "            if isinstance(record[split_feature], int):\n",
    "                if record[split_feature] >= split_val:\n",
    "                    tree = ltree\n",
    "                else:\n",
    "                    tree = rtree\n",
    "            else:\n",
    "                if record[split_feature] == split_val:\n",
    "                    tree = ltree\n",
    "                else:\n",
    "                    tree = rtree\n",
    "        return tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0IvbriYy5yN"
   },
   "outputs": [],
   "source": [
    "# helper function. You don't have to modify it\n",
    "def DecisionTreeEvalution(dt,X,y, verbose=True):\n",
    "\n",
    "    # Make predictions\n",
    "    # For each test sample X, use our fitting dt classifer to predict\n",
    "    y_predicted = []\n",
    "    for record in X: \n",
    "        y_predicted.append(dt.predict(record))\n",
    "\n",
    "    # Comparing predicted and true labels\n",
    "    results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True)) / float(len(results))\n",
    "    if verbose:\n",
    "        print(\"accuracy: %.4f\" % accuracy)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us use the Decision Tree to build a classifier and then to make predictions.\n",
    "First load training and test dataset. Please do not modify the code in the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function. You don't have to modify it\n",
    "data_test = pd.read_csv(\"hw4_data_test.csv\")\n",
    "data_valid = pd.read_csv(\"hw4_data_valid.csv\")\n",
    "data_train = pd.read_csv(\"hw4_data_train.csv\")\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital-status', 'occupation', \n",
    "                   'relationship', 'race', 'sex', 'native-country']\n",
    "numerical = ['age', 'fnlwgt', 'education-num','capital-gain', 'capital-loss',\n",
    "                'hours-per-week']\n",
    "\n",
    "for feature in categorical:\n",
    "        le = LabelEncoder()\n",
    "        data_train[feature] = le.fit_transform(data_train[feature])\n",
    "        data_test[feature] = le.fit_transform(data_test[feature])\n",
    "        \n",
    "X_train = pd.concat([data_train[categorical], data_train[numerical]], axis=1)\n",
    "y_train = data_train['high-income']\n",
    "X_test = pd.concat([data_test[categorical], data_test[numerical]], axis=1)\n",
    "y_test = data_test['high-income']\n",
    "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "\n",
    "for feature in categorical:\n",
    "        le = LabelEncoder()\n",
    "        data_valid[feature] = le.fit_transform(data_valid[feature])  \n",
    "        \n",
    "X_valid = pd.concat([data_valid[categorical], data_valid[numerical]], axis=1)\n",
    "y_valid = data_valid['high-income']\n",
    "X_valid, y_valid = np.array(X_valid), np.array(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the training data to fit your decision tree. It may take 3 - 10 minutes for fully fitting the tree. You may adjust the max_depth parameter to save some of your time(This may affect accuracy)\n",
    ". We will not take running time into account when grading this part. You should reach at least 80% accuracy on test set to receive full credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the decision tree\n",
      "evaluating the decision tree\n",
      "accuracy: 0.8337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8336832799204332"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a decision tree.\n",
    "max_depth = None\n",
    "dt = MyDecisionTree(max_depth)\n",
    "\n",
    "# Building a tree\n",
    "print(\"fitting the decision tree\")\n",
    "dt.fit(X_train, y_train, 0)\n",
    "\n",
    "# Evaluating the decision tree\n",
    "print(\"evaluating the decision tree\")\n",
    "DecisionTreeEvalution(dt,X_test,y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# Part 3\n",
    "## This part is challenging so bonus for both undergrads and grads : Pruning (10 Pts)\n",
    "\n",
    "In order to avoid overfitting, you can: 1. Acquire more training data; 2. Remove irrelevant attributes; 3. Grow full tree, then post-prune; 4. Ensemble learning. \n",
    "\n",
    "In this bonus part, you are going to apply reduced error post-pruning to prune the fully grown tree.\n",
    "The idea is basically about, starting at the leaves, each node is replaced with its most popular class. If the prediction accuracy is not affected then the change is kept. You may also try recursive function to apply the post-pruning. Please notice we use validation set to get the accuracy for each node during the pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define the post-pruning function\n",
    "def pruning(dt, X, y):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    1. Prune the full grown decision tress recursively.  \n",
    "    2. Classify examples in validation set.\n",
    "    3. For each node: \n",
    "    3.1 Sum errors over the entire subtree. You may want to use the helper function \"DecisionTreeEvalution\".\n",
    "    3.2 Calculate the error on same example if converted to a leaf with majority class label. \n",
    "    You may want to use the helper function \"DecisionTreeError\".\n",
    "    4. If error rate in the subtree is greater than in the single leaf, replace the whole subtree by a leaf node.\n",
    "    5. Return the pruned decision tree.\n",
    "    \"\"\"\n",
    "    #  Delete this line when you implement the function\n",
    "    raise NotImplementedError\n",
    "               \n",
    "def DecisionTreeError(y):\n",
    "    # helper function for calculating the error of the entire subtree if converted to a leaf with majority class label.\n",
    "    # You don't have to modify it  \n",
    "    num_ones = np.sum(y)\n",
    "    num_zeros = len(y) - num_ones\n",
    "    return 1.0 - max(num_ones, num_zeros) / float(len(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should make use of the decision tree you trained in part1. Make sure to let it have 20 or greater depths. Due the unbalance of our dataset, the post-pruning does not necessarily have better accuracy on test set. We will award full credits as long as your implementation is correct the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7558023872679045"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function. You don't have to modify it.\n",
    "# pruning the full grown decision tree using validation set \n",
    "# dt should be a decision tree object that has been fully trained\n",
    "dt_pruned=pruning(dt, X_test, y_test)\n",
    "\n",
    "# Evaluate the decision tree using test set \n",
    "DecisionTreeEvalution(dt_pruned, X_valid, y_valid, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aGIzP6dtY-Y"
   },
   "source": [
    "## Part 4: Random Forests [35pts]\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of examples almost inevitably leads to **overfitting**. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging').\n",
    "\n",
    "A Random Forest is a collection of decision trees, built as follows:\n",
    "\n",
    "1) For every tree we're going to build:\n",
    "\n",
    "    a) Subsample the examples with replacement. Note that in this question, the size of the subsample data is equal to the original dataset. \n",
    "    \n",
    "    b) From the subsamples in a), choose attributes at random to learn on in accordance with a provided attribute subsampling rate. Based on what it was mentioned in the class, we randomly pick features in each split. We use a more general approach here to make the programming part easier. Let's randomly pick some features (70% percent of features) and grow the tree based on the pre-determined randomly selected features. Therefore, there is no need to find random features in each split.\n",
    "    \n",
    "    c) Fit a decision tree to the subsample of data we've chosen to a certain depth.\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "In RandomForests Class, \n",
    "1. X is assumed to be a matrix with num_training rows and num_features columns where num_training is the\n",
    "number of total records and num_features is the number of features of each record. \n",
    "\n",
    "2. y is assumed to be a vector of labels of length num_training.\n",
    "\n",
    "**NOTE:** Lookout for TODOs for the parts that needs to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n8GGVU7tYGh"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: For graduate student, you are required to use your own decision tree MyDecisionTree() to finish random forest.\n",
    "\"\"\"\n",
    "\n",
    "class RandomForest(object):\n",
    "    def __init__(self, n_estimators=50, max_depth=None, max_features=0.7):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initialization done here\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.bootstraps_row_indices = []\n",
    "        self.feature_indices = []\n",
    "        self.out_of_bag = []\n",
    "        self.decision_trees = [MyDecisionTree(max_depth=max_depth) for i in range(n_estimators)]\n",
    "        \n",
    "    def _bootstrapping(self, num_training, num_features):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        - Randomly select a sample dataset of size num_training with replacement from the original dataset. \n",
    "        - Randomly select certain number of features (num_features denotes the total number of features in X, \n",
    "          max_features denotes the percentage of features that are used to fit each decision tree) without replacement from the total number of features.\n",
    "        \n",
    "        Return:\n",
    "        - row_idx: the row indices corresponding to the row locations of the selected samples in the original dataset.\n",
    "        - col_idx: the column indices corresponding to the column locations of the selected features in the original feature list.\n",
    "        \n",
    "        Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        \"\"\" \n",
    "        row_idx = np.random.choice(num_training, num_training, replace=True)\n",
    "        feat_count = int(num_features*self.max_features)\n",
    "        col_idx = np.random.choice(num_features, feat_count, replace=False)\n",
    "        return row_idx, col_idx\n",
    "            \n",
    "    def bootstrapping(self, num_training,num_features):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.n_estimators):\n",
    "            total = set(list(range(num_training)))\n",
    "            row_idx, col_idx = self._bootstrapping(num_training, num_features)\n",
    "            total = total - set(row_idx)\n",
    "            self.bootstraps_row_indices.append(row_idx)\n",
    "            self.feature_indices.append(col_idx)\n",
    "            self.out_of_bag.append(total)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Train decision trees using the bootstrapped datasets.\n",
    "        Note that you need to use the row indices and column indices.\n",
    "        \"\"\"\n",
    "        dfx = pd.DataFrame(X)\n",
    "        y = np.array(y)\n",
    "        self.bootstrapping(dfx.shape[0], X.shape[1])\n",
    "        bags = zip(self.decision_trees, self.bootstraps_row_indices, self.feature_indices)\n",
    "        for tree, row_idx, col_idx in bags:\n",
    "            X_sample = dfx.iloc[row_idx][col_idx].values\n",
    "            y_sample = y[row_idx]\n",
    "            tree.fit(X_sample, y_sample, 0)\n",
    "    \n",
    "    def OOB_score(self, X, y):\n",
    "        # helper function. You don't have to modify it\n",
    "        accuracy = []\n",
    "        for i in range(len(X)):\n",
    "            predictions = []\n",
    "            for t in range(self.n_estimators):\n",
    "                if i in self.out_of_bag[t]:\n",
    "                    predictions.append(self.decision_trees[t].predict(X[i][self.feature_indices[t]]))\n",
    "            if len(predictions) > 0:\n",
    "                accuracy.append(np.sum(predictions == y[i]) / float(len(predictions)))\n",
    "        return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "AC1-lWuct2wj",
    "outputId": "006bf714-6c0a-4d3e-f695-c1b326f0670d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8335\n",
      "time: 18:34\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: \n",
    "n_estimators defines how many decision trees are fitted for the random forest (at least 10). \n",
    "max_depth defines a stop condition when the tree reaches to a certain depth.\n",
    "max_features controls the percentage of features that are used to fit each decision tree.\n",
    "Tune these three parameters to achieve a better accuracy (Required min. accuracy is 0.83.)\n",
    "The random forest fitting may take 5 - 15 minutes. We will not take running time into account when grading this part.\n",
    "\"\"\"\n",
    "n_estimators = 10\n",
    "max_depth = 8\n",
    "max_features = 0.8\n",
    "\n",
    "random_forest = RandomForest(n_estimators, max_depth, max_features)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "accuracy=random_forest.OOB_score(X_train, y_train)\n",
    "\n",
    "print(\"accuracy: %.4f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: SVM (30 Pts)\n",
    "\n",
    "### 5.1 Fitting an SVM classifier by hand (20 Pts)\n",
    "\n",
    "Consider a dataset with 2 points in 1-dimensional space: $(x_1 = 0, y_1 = −1)$ and $(x_2 = \\sqrt{2}, y_2 = 1)$.\n",
    "\n",
    "Consider mapping each point to 3-dimensional space using the feature vector $\\phi(x) = [1,\\sqrt{2}x, x^2]$. (This is equivalent to using a second order polynomial kernel.) The max margin classifier has the form\n",
    "\n",
    "$$min ||\\mathbf{\\theta}||^2 s.t.$$\n",
    "\n",
    "$$y_1(\\phi(x_1)\\mathbf{\\theta} + b) ≥ 1 $$\n",
    "\n",
    "$$y_2(\\phi(x_2)\\mathbf{\\theta}+ b) ≥ 1 $$\n",
    "\n",
    "**Hint:** $\\phi(x_1)$ and $\\phi(x_2)$ are the suppport vectors. We have already given you the solution for the suppport vectors and you need to calculate back the parameters. Margin is equal to $\\frac{1}{||\\mathbf{\\theta}||}$ and full margin is equal to $\\frac{2}{||\\mathbf{\\theta}||}$.\n",
    "\n",
    "(1) Find a vector parallel to the optimal vector $\\mathbf{\\theta}$. (4pts)\n",
    "\n",
    "$\\theta$ is perpendicular to the support vectors and therefore parallel to the vector between the support vectors. This means that $\\theta$ is parallel to $\\phi(x_2) - \\phi(x_1)=[1,2,2]-[1,0,0] = [0,2,2]$.\n",
    "\n",
    "(2) Calculate the value of the margin achieved by this $\\mathbf{\\theta}$? (4pts)\n",
    "\n",
    "The margin is half the distance between the two points such that $\\frac{1}{||\\theta||}=\\frac{||\\phi(x_2)-\\phi(x_1)||}{2}=\\frac{||[0,2,2]||}{2}=\\frac{\\sqrt 8}{2}=\\sqrt 2$.\n",
    "\n",
    "(3) Solve for $\\mathbf{\\theta}$, given that the margin is equal to $1/||\\mathbf{\\theta}||$. (4pts)\n",
    "\n",
    "$\\frac{1}{||\\theta||}=\\frac{||\\phi(x_2)-\\phi(x_1)||}{2} \\: \\Rightarrow \\: ||\\theta||=\\frac{2}{||\\phi(x_2)-\\phi(x_1)||} \\: \\Rightarrow \\: \\theta = \\frac{2}{\\phi(x_2)-\\phi(x_1)}=[0,1,1]$\n",
    "\n",
    "(4) Solve for $b$ using your value for $\\mathbf{\\theta}$. (4pts)\n",
    "\n",
    "$\\phi(x_1)\\theta+b+\\phi(x_2)\\theta+b=0 \\: \\Rightarrow \\: [1,0,0][0,1,1]+b+[1,2,2][0,1,1]+b=0 \\: \\Rightarrow \\: b=-\\frac{5}{2}$\n",
    "\n",
    "(5) Write down the form of the discriminant function $f(x) = \\phi(x)\\mathbf{\\theta}+b$ as an explicit function of $x$. (4pts)\n",
    "\n",
    "$f(x)=\\phi(x)+b=[1,\\sqrt{2}x,x^2][0,1,1]-\\frac{5}{2}=\\sqrt{2}x+x^2-\\frac{5}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SVM Kernel (10 Pts)\n",
    "\n",
    "Suppose we have a dataset in 2-dimensional space which consists of 1 data point $(x_1=2, x_2=2)$ with the positive label and 4 data points $(x_1=1, x_2=1), (x_1=3, x_2=1), (x_1=3, x_2=3), (x_1=1, x_2=3)$ with the negative labels.\n",
    "\n",
    "(1) Find a feature map, which will map the original 2-dimensional data points to the feature space so that the positive samples and the negative samples are linearly separable with each other. Draw the dataset after mapping in the feature space. (5pts)\n",
    "\n",
    "(2) In your plot above, draw the decision boundary given by hard-margin linear SVM. Mark the corresponding support vectors. (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
